{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dobby/AttnGAN/code\n"
     ]
    }
   ],
   "source": [
    "cd /home/dobby/AttnGAN/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dobby/AttnGAN/code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dobby/AttnGAN/code/miscc/config.py:103: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n",
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'DAMSM',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 64,\n",
      "         'GF_DIM': 128,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 0,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 15},\n",
      " 'TRAIN': {'BATCH_SIZE': 48,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'NET_E': '',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 1.0},\n",
      "           'SNAPSHOT_INTERVAL': 5},\n",
      " 'TREE': {'BASE_SIZE': 299, 'BRANCH_NUM': 1},\n",
      " 'WORKERS': 1}\n",
      "Load filenames from: ../data/coco/train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco/test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n",
      "27297 5\n",
      "Load filenames from: ../data/coco/train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco/test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n",
      "/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/dobby/.local/lib/python3.6/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "/home/dobby/AttnGAN/code/miscc/losses.py:125: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
      "  similarities.data.masked_fill_(masks, -float('inf'))\n",
      "/home/dobby/AttnGAN/code/miscc/losses.py:52: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
      "  scores0.data.masked_fill_(masks, -float('inf'))\n",
      "pretrain_DAMSM.py:97: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  cfg.TRAIN.RNN_GRAD_CLIP)\n",
      "| epoch   0 |     0/ 1724 batches | ms/batch 12.54 | s_loss  0.02  0.02 | w_loss  0.03  0.02\n",
      "| epoch   0 |   200/ 1724 batches | ms/batch 514.56 | s_loss  2.61  2.61 | w_loss  2.80  2.61\n",
      "| epoch   0 |   400/ 1724 batches | ms/batch 500.65 | s_loss  1.64  1.64 | w_loss  1.68  1.61\n",
      "| epoch   0 |   600/ 1724 batches | ms/batch 504.73 | s_loss  1.38  1.39 | w_loss  1.39  1.36\n",
      "| epoch   0 |   800/ 1724 batches | ms/batch 503.85 | s_loss  1.24  1.26 | w_loss  1.25  1.23\n",
      "| epoch   0 |  1000/ 1724 batches | ms/batch 506.68 | s_loss  1.17  1.20 | w_loss  1.15  1.17\n",
      "| epoch   0 |  1200/ 1724 batches | ms/batch 514.78 | s_loss  1.12  1.14 | w_loss  1.08  1.08\n",
      "| epoch   0 |  1400/ 1724 batches | ms/batch 502.33 | s_loss  1.07  1.09 | w_loss  1.04  1.04\n",
      "| epoch   0 |  1600/ 1724 batches | ms/batch 515.79 | s_loss  1.06  1.08 | w_loss  1.02  1.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   0 | valid loss  2.03  1.88 | lr 0.00200|\n",
      "-----------------------------------------------------------------------------------------\n",
      "Save G/Ds models.\n",
      "| epoch   1 |     0/ 1724 batches | ms/batch  4.44 | s_loss  0.01  0.01 | w_loss  0.01  0.01\n",
      "| epoch   1 |   200/ 1724 batches | ms/batch 505.16 | s_loss  0.99  1.02 | w_loss  0.93  0.94\n",
      "| epoch   1 |   400/ 1724 batches | ms/batch 507.23 | s_loss  0.95  0.97 | w_loss  0.89  0.90\n",
      "| epoch   1 |   600/ 1724 batches | ms/batch 501.97 | s_loss  0.94  0.96 | w_loss  0.87  0.89\n",
      "| epoch   1 |   800/ 1724 batches | ms/batch 516.89 | s_loss  0.92  0.94 | w_loss  0.85  0.87\n",
      "| epoch   1 |  1000/ 1724 batches | ms/batch 507.95 | s_loss  0.93  0.96 | w_loss  0.86  0.88\n",
      "| epoch   1 |  1200/ 1724 batches | ms/batch 497.14 | s_loss  0.92  0.94 | w_loss  0.84  0.85\n",
      "| epoch   1 |  1400/ 1724 batches | ms/batch 504.41 | s_loss  0.89  0.92 | w_loss  0.81  0.82\n",
      "| epoch   1 |  1600/ 1724 batches | ms/batch 503.19 | s_loss  0.88  0.91 | w_loss  0.81  0.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   1 | valid loss  1.79  1.63 | lr 0.00196|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |     0/ 1724 batches | ms/batch  4.26 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   2 |   200/ 1724 batches | ms/batch 503.44 | s_loss  0.85  0.88 | w_loss  0.77  0.78\n",
      "| epoch   2 |   400/ 1724 batches | ms/batch 500.08 | s_loss  0.84  0.87 | w_loss  0.75  0.78\n",
      "| epoch   2 |   600/ 1724 batches | ms/batch 493.46 | s_loss  0.83  0.86 | w_loss  0.77  0.77\n",
      "| epoch   2 |   800/ 1724 batches | ms/batch 502.76 | s_loss  0.85  0.87 | w_loss  0.77  0.77\n",
      "| epoch   2 |  1000/ 1724 batches | ms/batch 505.24 | s_loss  0.85  0.87 | w_loss  0.76  0.78\n",
      "| epoch   2 |  1200/ 1724 batches | ms/batch 511.63 | s_loss  0.85  0.88 | w_loss  0.76  0.78\n",
      "| epoch   2 |  1400/ 1724 batches | ms/batch 496.87 | s_loss  0.83  0.85 | w_loss  0.74  0.75\n",
      "| epoch   2 |  1600/ 1724 batches | ms/batch 498.19 | s_loss  0.82  0.84 | w_loss  0.72  0.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   2 | valid loss  1.65  1.42 | lr 0.00192|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |     0/ 1724 batches | ms/batch  4.36 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   3 |   200/ 1724 batches | ms/batch 511.56 | s_loss  0.80  0.83 | w_loss  0.69  0.71\n",
      "| epoch   3 |   400/ 1724 batches | ms/batch 514.80 | s_loss  0.80  0.82 | w_loss  0.71  0.71\n",
      "| epoch   3 |   600/ 1724 batches | ms/batch 505.36 | s_loss  0.79  0.82 | w_loss  0.68  0.70\n",
      "| epoch   3 |   800/ 1724 batches | ms/batch 500.88 | s_loss  0.82  0.85 | w_loss  0.71  0.73\n",
      "| epoch   3 |  1000/ 1724 batches | ms/batch 501.79 | s_loss  0.79  0.82 | w_loss  0.70  0.72\n",
      "| epoch   3 |  1200/ 1724 batches | ms/batch 513.30 | s_loss  0.80  0.82 | w_loss  0.70  0.72\n",
      "| epoch   3 |  1400/ 1724 batches | ms/batch 500.69 | s_loss  0.79  0.82 | w_loss  0.69  0.70\n",
      "| epoch   3 |  1600/ 1724 batches | ms/batch 501.12 | s_loss  0.80  0.82 | w_loss  0.70  0.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end epoch   3 | valid loss  1.61  1.36 | lr 0.00188|\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |     0/ 1724 batches | ms/batch  4.39 | s_loss  0.00  0.00 | w_loss  0.00  0.00\n",
      "| epoch   4 |   200/ 1724 batches | ms/batch 506.04 | s_loss  0.77  0.79 | w_loss  0.65  0.68\n",
      "| epoch   4 |   400/ 1724 batches | ms/batch 512.83 | s_loss  0.76  0.79 | w_loss  0.65  0.66\n",
      "| epoch   4 |   600/ 1724 batches | ms/batch 500.61 | s_loss  0.76  0.79 | w_loss  0.65  0.67\n",
      "| epoch   4 |   800/ 1724 batches | ms/batch 503.03 | s_loss  0.76  0.79 | w_loss  0.65  0.66\n",
      "| epoch   4 |  1000/ 1724 batches | ms/batch 502.09 | s_loss  0.76  0.78 | w_loss  0.65  0.66\n",
      "| epoch   4 |  1200/ 1724 batches | ms/batch 504.45 | s_loss  0.77  0.80 | w_loss  0.66  0.68\n",
      "| epoch   4 |  1400/ 1724 batches | ms/batch 506.51 | s_loss  0.78  0.80 | w_loss  0.66  0.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "-----------------------------------------------------------------------------------------\r\n",
      "Exiting from training early\r\n"
     ]
    }
   ],
   "source": [
    "!python3 pretrain_DAMSM.py --cfg cfg/DAMSM/coco.yml --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 25 12:52:13 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 39%   60C    P2   151W / 260W |  18024MiB / 24220MiB |     50%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
      "[GCC 8.4.0] on linux\n",
      "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
      ">>> \n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "KeyboardInterrupt\n",
      ">>> "
     ]
    }
   ],
   "source": [
    "!python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
