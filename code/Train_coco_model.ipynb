{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dobby/AttnGAN/code\n"
     ]
    }
   ],
   "source": [
    "cd /home/dobby/AttnGAN/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dobby/AttnGAN/code'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dobby/AttnGAN/code/miscc/config.py:103: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n",
      "Using config:\n",
      "{'B_VALIDATION': False,\n",
      " 'CONFIG_NAME': 'glu-gan2',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'coco',\n",
      " 'DATA_DIR': '../data/coco',\n",
      " 'GAN': {'B_ATTENTION': True,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 96,\n",
      "         'GF_DIM': 48,\n",
      "         'R_NUM': 3,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': 0,\n",
      " 'RNN_TYPE': 'LSTM',\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 5, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 12},\n",
      " 'TRAIN': {'BATCH_SIZE': 14,\n",
      "           'B_NET_D': True,\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'ENCODER_LR': 0.0002,\n",
      "           'FLAG': True,\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 120,\n",
      "           'NET_E': '../DAMSMencoders/coco/text_encoder100.pth',\n",
      "           'NET_G': '',\n",
      "           'RNN_GRAD_CLIP': 0.25,\n",
      "           'SMOOTH': {'GAMMA1': 4.0,\n",
      "                      'GAMMA2': 5.0,\n",
      "                      'GAMMA3': 10.0,\n",
      "                      'LAMBDA': 50.0},\n",
      "           'SNAPSHOT_INTERVAL': 5},\n",
      " 'TREE': {'BASE_SIZE': 64, 'BRANCH_NUM': 3},\n",
      " 'WORKERS': 4}\n",
      "Load filenames from: ../data/coco/train/filenames.pickle (82783)\n",
      "Load filenames from: ../data/coco/test/filenames.pickle (40470)\n",
      "Load from:  ../data/coco/captions.pickle\n",
      "/home/dobby/.local/lib/python3.6/site-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
      "Load pretrained model from  https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\n",
      "Load image encoder from: ../DAMSMencoders/coco/image_encoder100.pth\n",
      "/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "Load text encoder from: ../DAMSMencoders/coco/text_encoder100.pth\n",
      "# of netsD 3\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 140, in <module>\n",
      "    algo.train()\n",
      "  File \"/home/dobby/AttnGAN/code/trainer.py\", line 262, in train\n",
      "    fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dobby/AttnGAN/code/model.py\", line 453, in forward\n",
      "    self.h_net3(h_code2, c_code, word_embs, mask)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dobby/AttnGAN/code/model.py\", line 390, in forward\n",
      "    out_code = self.upsample(out_code)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\", line 117, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 423, in forward\n",
      "    return self._conv_forward(input, self.weight)\n",
      "  File \"/home/dobby/.local/lib/python3.6/site-packages/torch/nn/modules/conv.py\", line 420, in _conv_forward\n",
      "    self.padding, self.dilation, self.groups)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 336.00 MiB (GPU 0; 23.65 GiB total capacity; 4.53 GiB already allocated; 337.31 MiB free; 4.61 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "!python3 main.py --cfg cfg/coco_attn2.yml --gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 24 15:08:41 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro RTX 6000     Off  | 00000000:3B:00.0 Off |                  Off |\r\n",
      "| 39%   65C    P2   251W / 260W |  18024MiB / 24220MiB |     95%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
